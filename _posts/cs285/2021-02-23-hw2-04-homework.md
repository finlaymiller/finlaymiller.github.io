---
title: "Homework 2"
permalink: cs285/hw2/solutions
date: 2021-02-23T09:00:0-04:00
excerpt: "Summary of my work on homework 2"
categories:
  - CS285
tags:
  - CS285
  - Machine Learning
  - Deep Reinforcement Learning
toc: true
toc_sticky: true
sidebar:
  nav: "cs285"
---

All files for this homework are in [my GitHub](https://github.com/finlaymiller/homework_fall2020/tree/master/hw2).

## Experiment 1

Data was collected using the following scripts:

```bash
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 -dsa --exp_name q1_sb_no_rtg_dsa
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 -rtg -dsa --exp_name q1_sb_rtg_dsa
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 -rtg --exp_name q1_sb_rtg_na
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 -dsa --exp_name q1_lb_no_rtg_dsa
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 -rtg -dsa --exp_name q1_lb_rtg_dsa
python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 -rtg --exp_name q1_lb_rtg_na
```

### Small Batch Experiments

![small batch](/assets/img/cs285/hw2/q1a.png)

It is clear that Reward to Go has more of an effect on the results than advantage standardization. Both together seems to be better than just Reward to Go on it's own.

### Large Batch Experiments

![large batch](/assets/img/cs285/hw2/q1b.png)

Unsurprising that the model performs better with a larger batch size. Similar to with the small batches, Reward to Go is proven beneficial.

## Experiment 2

![inverted pendulum](/assets/img/cs285/hw2/q2.png)

A batch size of 2000 and a learning rate of 0.02 seemed to be a good balancing point. The command used was:

```bash
python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \
--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 2000 -lr 0.02 \
-rtg --exp_name q2_b2000_r0.02
```

## Experiment 3

![lunar lander](/assets/img/cs285/hw2/q3.png)

## Experiment 4

![half cheetah 1](/assets/img/cs285/hw2/q4a.png)
![half cheetah 2](/assets/img/cs285/hw2/q4b.png)

## Thoughts

Pretty cool to see directly the different techniques' effect on the model, even if the results were pretty much as expected. Definitely more fun & interesting than the first assignment.
